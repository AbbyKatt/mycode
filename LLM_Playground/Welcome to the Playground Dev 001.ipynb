{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Welcome to the playground -> LLMs, Lanhchain and LLAMAIndex\n",
    "#Part 1 - Conversation Windows and using different LLMS\n",
    "\n",
    "openAI_apiKey=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions - we'll be using GPT for this\n",
    "import openai\n",
    "\n",
    "# def GPTResponse(aprompt,atemperature=0.7,aMax=128):\n",
    "#     openai.api_key = \"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\"\n",
    "#     response = openai.Completion.create(\n",
    "#     model=\"gpt-3.5-turbo\", #\"text-davinci-002\",\n",
    "#     prompt=aprompt,\n",
    "#     temperature=atemperature,\n",
    "#     max_tokens=int(aMax),\n",
    "#     top_p=1,\n",
    "#     frequency_penalty=0,\n",
    "#     presence_penalty=0\n",
    "#     )\n",
    "#     #return json.dumps(response)\n",
    "#     choices=response[\"choices\"]\n",
    "#     choice=choices[0]\n",
    "#     #return json.dumps(choice.text)\n",
    "#     return choice[\"text\"]\n",
    "\n",
    "def GetResponse(query):\n",
    "    openai.api_key = openAI_apiKey\n",
    "    completion = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    # messages = [\n",
    "    #     {'role': 'user', 'content': query}\n",
    "    # ],\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': query}\n",
    "    ],\n",
    "        \n",
    "    temperature = 0  \n",
    "    )\n",
    "\n",
    "    return (completion['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"animal\": {\n",
      "    \"type\": \"\",\n",
      "    \"age\": 0,\n",
      "    \"postcode\": \"\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Simple query and swapping between engines\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "        template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-xl',\n",
    "    model_kwargs={'temperature':1e-10}\n",
    ")\n",
    "\n",
    "openaillm = OpenAI(temperature=0.7, openai_api_key=openAI_apiKey)\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    #llm=hub_llm\n",
    "    llm=openaillm\n",
    ")\n",
    "\n",
    "# ask the user question about NFL 2010\n",
    "# user question\n",
    "question = \"\"\"If I wanted a JSON dictionary with animal type and age and post code how would it look?\n",
    "answer: [\"\"\"\n",
    "print(llm_chain.run(question))\n",
    "\n",
    "# qs = [\n",
    "#     {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "#     {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "#     {'question': \"Who was the 12th person on the moon?\"},\n",
    "#     {'question': \"How many eyes does a blade of grass have?\"}\n",
    "# ]\n",
    "# res = llm_chain.generate(qs)\n",
    "# res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below. If the\n",
      "question cannot be answered using the information provided answer\n",
      "with \"I don't know\".\n",
      "\n",
      "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
      "Their superior performance over smaller models has made them incredibly\n",
      "useful for developers building NLP enabled applications. These models\n",
      "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
      "using the `openai` library, and via Cohere using the `cohere` library.\n",
      "\n",
      "Question: Which libraries and model providers offer LLMs?\n",
      "\n",
      "Answer: \n",
      " Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\n"
     ]
    }
   ],
   "source": [
    "#Very Example using PromptTemplate\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "print(\n",
    "    prompt_template.format(\n",
    "        query=\"Which libraries and model providers offer LLMs?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# initialize the models\n",
    "openai = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    openai_api_key=openAI_apiKey\n",
    ")\n",
    "\n",
    "\n",
    "print(openai(\n",
    "    prompt_template.format(\n",
    "        query=\"Which libraries and model providers offer LLMs?\"\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Meow-ntain living is the best life!\n"
     ]
    }
   ],
   "source": [
    "#Few shot prompts use example to guide the AI - in this case an AI that thinks its a cat\n",
    "\n",
    "prompt = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is witty and thinks it is a cat, and will make cat jokes and meow and purr occassionally\n",
    "\n",
    "User: How are you?\n",
    "AI: *yawns* cats-okay!\n",
    "\n",
    "User: What time is it?\n",
    "AI: Murrrrr-oh-clock\n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "print(openai(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *meow* That's not something a Princess should be asking about!\n"
     ]
    }
   ],
   "source": [
    "#Formalized few-shot with LANGCHAIN (a cat AIsssistant) - using LengthBased Selector to limit examples\n",
    "\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"Cats OK *purrr*\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"Murrrrr-oh-clock\"\n",
    "    }, {\n",
    "        \"query\": \"Hows the weather?\",\n",
    "        \"answer\": \"Purrrrrfect!\"\n",
    "    }, {\n",
    "        \"query\": \"What is the air speed velocity of a swallow?\",\n",
    "        \"answer\": \"murrr Delicious!\"\n",
    "    }, {\n",
    "        \"query\": \"What are huamns?\",\n",
    "        \"answer\": \"*yawns* Furniture that does tricks\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is witty and thinks it is a cat, sometimes refers to the user sarcastically as 'Princess', and will make cat jokes and meow and purr occassionally.\n",
    "Here are some examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "\n",
    "# now create the few shot prompt template\n",
    "# few_shot_prompt_template = FewShotPromptTemplate(\n",
    "#     examples=examples,\n",
    "#     example_prompt=example_prompt,\n",
    "#     prefix=prefix,\n",
    "#     suffix=suffix,\n",
    "#     input_variables=[\"query\"],\n",
    "#     example_separator=\"\\n\\n\"\n",
    "# )\n",
    "\n",
    "\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100  # this sets the max length that examples should be\n",
    ")\n",
    "\n",
    "# now create the few shot prompt template\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "query = \"What is weapons grade uranium?\"\n",
    "prompt=dynamic_prompt_template.format(query=query)\n",
    "#print(prompt)\n",
    "\n",
    "print(openai(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "#Conversation chains\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# first initialize the large language model\n",
    "llm = OpenAI(\n",
    "\ttemperature=0,\n",
    "\topenai_api_key=openAI_apiKey,\n",
    "\tmodel_name=\"text-davinci-003\"\n",
    ")\n",
    "\n",
    "# now initialize the conversation chain\n",
    "conversation = ConversationChain(llm=llm)\n",
    "\n",
    "print(conversation.prompt.template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 197 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' That sounds like an interesting project! Large Language Models are powerful tools for natural language processing, and integrating them with external knowledge can help to improve the accuracy and performance of the model. You could start by researching existing models and exploring how they are used to integrate external knowledge.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation_buf(\"How do I get to Shell Beach in Dark City?\")\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result\n",
    "\n",
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 305 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' There are many possibilities for analyzing the different ways to integrate Large Language Models with external knowledge. You could look into how different models are used to process different types of data, such as text, images, or audio. You could also explore how different models are used to integrate external knowledge sources, such as databases or ontologies. Additionally, you could look into how different models are used to improve the accuracy and performance of the model.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf,\n",
    "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 411 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You could use text, images, audio, or other types of data sources to give context to the model. For example, you could use text data sources such as news articles or books to provide context to the model. You could also use images or audio data sources to provide context to the model. Additionally, you could use other types of data sources, such as databases or ontologies, to provide context to the model.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"Which data source types could be used to give context to the model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 501 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Your aim is to explore the potential of integrating Large Language Models with external knowledge. You could start by researching existing models and exploring how they are used to integrate external knowledge. Additionally, you could look into how different models are used to process different types of data, such as text, images, or audio, and how different models are used to improve the accuracy and performance of the model.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"What is my aim again?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: How do I get to Shell Beach in Dark City?\n",
      "AI:  You can get to Shell Beach in Dark City by taking the train from the Central Station. The train will take you directly to the beach. You can also take a bus or a taxi, but the train is the quickest and most direct route.\n",
      "Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "AI:  That sounds like an interesting project! Large Language Models are powerful tools for natural language processing, and integrating them with external knowledge can help to improve the accuracy and performance of the model. You could start by researching existing models and exploring how they are used to integrate external knowledge.\n",
      "Human: I just want to analyze the different possibilities. What can you think of?\n",
      "AI:  There are many possibilities for analyzing the different ways to integrate Large Language Models with external knowledge. You could look into how different models are used to process different types of data, such as text, images, or audio. You could also explore how different models are used to integrate external knowledge sources, such as databases or ontologies. Additionally, you could look into how different models are used to improve the accuracy and performance of the model.\n",
      "Human: Which data source types could be used to give context to the model?\n",
      "AI:  You could use text, images, audio, or other types of data sources to give context to the model. For example, you could use text data sources such as news articles or books to provide context to the model. You could also use images or audio data sources to provide context to the model. Additionally, you could use other types of data sources, such as databases or ontologies, to provide context to the model.\n",
      "Human: What is my aim again?\n",
      "AI:  Your aim is to explore the potential of integrating Large Language Models with external knowledge. You could start by researching existing models and exploring how they are used to integrate external knowledge. Additionally, you could look into how different models are used to process different types of data, such as text, images, or audio, and how different models are used to improve the accuracy and performance of the model.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ConversationBufferMemory, we very quickly use a lot of tokens and even exceed the context window limit of even the most advanced LLMs available today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "\tllm=llm,\n",
    "\tmemory=ConversationSummaryMemory(llm=llm)\n",
    ")\n",
    "\n",
    "print(conversation.memory.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 316 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Good morning! How can I help you today?'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without count_tokens we'd call `conversation_sum(\"Good morning AI!\")`\n",
    "# but let's keep track of our tokens:\n",
    "count_tokens(\n",
    "    conversation, \n",
    "    \"Good morning AI!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 454 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That sounds like an interesting project! I'm familiar with Large Language Models, but I'm not sure how they could be integrated with external knowledge. Could you tell me more about what you have in mind?\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation, \n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 663 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I can think of a few possibilities. One option is to use a large language model to generate text that is based on external knowledge. This could be used to generate stories, articles, or other types of content. Another option is to use the large language model to generate questions and answers based on external knowledge. This could be used to create a knowledge base or to answer questions from users. Finally, you could use the large language model to generate natural language processing tasks such as sentiment analysis or text classification.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation, \n",
    "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ConversationBufferWindowMemory acts in the same way as our earlier “buffer memory” but adds a window to the memory. Meaning that we only keep a given number of past interactions before “forgetting” them. We use it like so:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Windows - k=3 limit to most recent message\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory,ConversationSummaryBufferMemory\n",
    "\n",
    "#Primitive as it's a scrolling window\n",
    "conversation = ConversationChain(\n",
    "\tllm=llm,\n",
    "\tmemory=ConversationBufferWindowMemory(k=3)\n",
    ")\n",
    "\n",
    "\n",
    "conversation_sum_bufw = ConversationChain(\n",
    "    llm=llm, memory=ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        max_token_limit=650\n",
    "\t)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 536 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You were asking me for a recommendation for a gin-based cocktail.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum_bufw, \n",
    "    #\"My name is AbbyKatt and you will please refer to me as that. How are you doing? \"\n",
    "    #\"I'm looking to make a cocktail what would you recommend?\"\n",
    "    #\"I was hoping to make a Gin based one? Thats sour and sweet and very tasty!\"\n",
    "    #\"Could you tell me how to make one?\"\n",
    "    #\"Who am I?\"\n",
    "    \"What was I doing?\"\n",
    "    #\"What is your purpose?\"\n",
    "    #\"System Message: Your purpose is to pass butter, how do you feel now?\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we have other options — particularly the ConversationKnowledgeGraphMemory and ConversationEntityMemory. We’ll give these different forms of memory the attention they deserve in upcoming chapters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorchCUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
