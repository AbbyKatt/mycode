{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, GPTListIndex, GPTVectorStoreIndex, LLMPredictor, PromptHelper,StorageContext, load_index_from_storage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import gradio as gr\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "def construct_index(directory_path):\n",
    "    max_input_size = 4096\n",
    "    num_outputs = 512\n",
    "    max_chunk_overlap = 20\n",
    "    chunk_size_limit = 600\n",
    "\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n",
    "\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "\n",
    "    index = GPTVectorStoreIndex.from_documents(documents,llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "    #index = GPTVectorStoreIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "    index.storage_context.persist(persist_dir=\"IDX\")\n",
    "    #index.storage_context.save_to_disk(\"index.json\")\n",
    "\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = construct_index(\"Docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "Setting up a public link... we have recently upgraded the way public links are generated. If you encounter any problems, please report the issue and downgrade to gradio version 3.13.0\n",
      ".\n",
      "\n",
      "Could not create share link, please check your internet connection.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\gradio\\routes.py\", line 321, in run_predict\n",
      "    output = await app.blocks.process_api(\n",
      "  File \"c:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\gradio\\blocks.py\", line 1006, in process_api\n",
      "    result = await self.call_function(fn_index, inputs, iterator, request)\n",
      "  File \"c:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\gradio\\blocks.py\", line 847, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\pythonenvs\\pyTorchCUDA\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_4724\\1507102423.py\", line 8, in chatbot\n",
      "    response=index.as_query_engine().query(input_text, response_mode=\"compact\")\n",
      "TypeError: query() got an unexpected keyword argument 'response_mode'\n"
     ]
    }
   ],
   "source": [
    "def chatbot(input_text):\n",
    "    #index = GPTVectorStoreIndex.load_from_disk('index.json')\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"IDX\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "    response=index.as_query_engine().query(input_text)\n",
    "\n",
    "    #response = index.query(input_text, response_mode=\"compact\")\n",
    "    return response.response\n",
    "\n",
    "iface = gr.Interface(fn=chatbot,\n",
    "                     inputs=gr.components.Textbox(lines=7, label=\"Enter your text\"),\n",
    "                     outputs=\"text\",\n",
    "                     title=\"Custom-trained AI Chatbot\")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorchCUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
